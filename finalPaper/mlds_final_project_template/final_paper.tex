\documentclass[12pt,twocolumn]{article}

% Packages
\usepackage{graphicx}
\usepackage{sectsty}
\usepackage{titlesec}
\usepackage{fancyhdr}
\usepackage{geometry}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{hyperref}
\usepackage{parskip}
\usepackage[numbers]{natbib}
\usepackage{tikz}
\usetikzlibrary{arrows.meta,positioning}

% Margins
\geometry{
  top=1in,
  bottom=1in,
  left=1in,
  right=1in
}

% Header & Footer
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{Road Distress Classification}
\fancyhead[R]{\thepage}

% Section Styling
\titleformat{\section}{\Large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\large\bfseries}{\thesubsection}{1em}{}

% Title, Authors, and Institutions
\title{
    \Huge \textbf{Final Project Report}\\[1em]
    \Large \textbf{Road Distress Classification Using Deep Learning with Mask-Enhanced Feature Learning}
}
\author{%
  \textbf{Dor Skoler}\\
  Reichman University\\
  \texttt{ID: 12345678}%
  \and
  \textbf{Guy Gazpan}\\
  Reichman University\\
  \texttt{ID: 208465757}%
}
\date{\today}

\begin{document}

% Create a full-width title block before the two-column content starts.
\twocolumn[{
\centering
\includegraphics[width=0.4\textwidth]{images/runi_logo.png}\\[1em]
{\LARGE Reichman University}\\[1em]
{\large Efi Arazi School of Computer Science}\\[1em]
{\large M.Sc. Program in Machine Learning and Data Science}\\[1em]
\vspace{2.5em}

{\Huge Final Project Report}\\[1.5em]
{\Large Road Distress Classification Using Deep Learning with Mask-Enhanced Feature Learning}\\[1em]
{\small \textbf{Dor Skoler} \quad \texttt{ID: 12345678} \quad \textbf{Guy Gazpan} \quad \texttt{ID: 208465757}}\\[1em]
{\small \textbf{Advisors:} TBD}\\[1em]
{\small \today}\\[1em]
\begin{flushleft}
\begin{abstract}
Road infrastructure monitoring is critical for public safety and maintenance planning. This project presents a comprehensive deep learning approach for automated road distress classification from images. We developed a multi-label classification system capable of identifying three key road conditions: damage (potholes, cracks), occlusion (shadows, debris), and crop (incomplete road views). Our approach leverages EfficientNet-B3 as the backbone architecture enhanced with road mask integration for focused feature learning. The system was trained and evaluated on a dataset of 18,173 road images with comprehensive data augmentation and regularization strategies. Our final model achieved 88.99\% overall accuracy, with particularly strong performance in occlusion detection (94.11\% accuracy) and excellent precision in crop detection (95.35\%). The mask-enhanced approach provided a 7.64\% improvement over baseline models by reducing background noise and focusing learning on relevant road areas. The system demonstrates real-time inference capability and includes a complete pipeline for practical deployment in road monitoring applications.
\end{abstract}
\end{flushleft}

\vspace{1em}
}]
\clearpage
\twocolumn

\section{Introduction}

Road infrastructure maintenance is a critical aspect of transportation management that directly impacts public safety, vehicle longevity, and economic efficiency. Traditional road inspection methods rely on manual surveys conducted by trained professionals, which are time-consuming, costly, and subject to human variability. The increasing demand for efficient and consistent road condition assessment has driven the development of automated systems for road distress detection and classification.

Computer vision and deep learning technologies have emerged as promising solutions for automated road condition monitoring \citet{he2016deep}. These approaches can process large volumes of road imagery quickly and consistently, providing objective assessments that complement human expertise. However, road distress classification presents unique challenges including varying lighting conditions, diverse road surfaces, complex backgrounds, and the need to distinguish between different types of distress conditions.

This project addresses the challenge of automated road distress classification by developing a deep learning system capable of identifying three critical road conditions: damage (including potholes and cracks), occlusion (shadows and debris that obscure road visibility), and crop (incomplete or poorly framed road views). Our approach combines modern convolutional neural network architectures with novel mask-enhanced feature learning to improve classification accuracy and reduce false positives from background elements.

The key contributions of this work include: (1) development of a comprehensive multi-label classification system for road distress detection, (2) integration of road segmentation masks to enhance feature learning, (3) extensive experimentation with data augmentation and regularization strategies, and (4) creation of a complete inference pipeline suitable for real-world deployment.

\section{Related Works}

Road condition assessment using computer vision has been an active area of research, with approaches ranging from traditional image processing techniques to modern deep learning methods. Early work in this domain focused on hand-crafted features and classical machine learning algorithms for detecting specific types of road distress \citet{bishop2006pattern}.

Recent advances in convolutional neural networks have significantly improved the accuracy and robustness of road distress detection systems. The introduction of deep residual networks \citet{he2016deep} enabled the training of much deeper architectures, leading to better feature extraction capabilities for complex visual tasks. EfficientNet architectures have further improved the efficiency-accuracy trade-offs in image classification tasks \citet{krizhevsky2012imagenet}.

Multi-label classification approaches have been explored for scenarios where multiple types of distress can occur simultaneously in a single image. These methods require careful consideration of label dependencies and appropriate loss functions to handle the multi-label nature of the problem effectively.

Segmentation-based approaches have shown promise in road infrastructure monitoring by first identifying road regions and then applying classification models to the segmented areas. This approach helps reduce false positives from background elements and focuses computational resources on relevant image regions.

\section{Data Overview}

Our dataset comprises 18,173 road images collected from various road conditions and environments. The dataset was carefully organized to ensure proper evaluation methodology and prevent data leakage between training and test sets.

\subsection{Dataset Composition}

The complete dataset was split into three subsets following a road-based partitioning strategy to ensure model evaluation on unseen road conditions:

\begin{itemize}
\item \textbf{Training Set}: 14,537 images (80.0\%)
\item \textbf{Validation Set}: 1,818 images (10.0\%)
\item \textbf{Test Set}: 1,818 images (10.0\%)
\end{itemize}

\subsection{Label Distribution}

The dataset exhibits varying class distributions across the three classification tasks:

\textbf{Damage Classification:}
\begin{itemize}
\item Damaged: 5,971 images (32.8\%)
\item Not Damaged: 12,196 images (67.2\%)
\end{itemize}

\textbf{Occlusion Classification:}
\begin{itemize}
\item Occluded: 3,476 images (19.1\%)
\item Not Occluded: 14,697 images (80.9\%)
\end{itemize}

\textbf{Crop Classification:}
\begin{itemize}
\item Cropped: 778 images (4.3\%)
\item Not Cropped: 17,395 images (95.7\%)
\end{itemize}

The analysis reveals significant class imbalance, particularly in the crop detection task where only 4.3\% of images are labeled as cropped. This imbalance necessitated special handling during training through weighted loss functions and targeted data augmentation strategies.

\subsection{Data Preprocessing}

All images were preprocessed using a comprehensive pipeline including:
\begin{itemize}
\item Resize to 256×256 pixels for consistent input dimensions
\item CLAHE (Contrast Limited Adaptive Histogram Equalization) for enhanced contrast
\item Bilateral filtering for noise reduction while preserving edges
\item Road mask generation using a pretrained U-Net model with ResNet34 encoder
\item Normalization using ImageNet statistics for transfer learning compatibility
\end{itemize}

\subsection{Preprocessing Variants Explored}

We evaluated several preprocessing strategies documented in the repository to improve robustness:
\begin{itemize}
\item \textbf{Road content filtering}: Retain images with at least 15\% road area using a pretrained U-Net road segmenter (threshold documented across experiments and pipeline docs).
\item \textbf{CLAHE + Bilateral filtering}: Contrast Limited Adaptive Histogram Equalization with edge-preserving smoothing (see preprocessing experiments on May 13, 2025).
\item \textbf{Augmentations}: Horizontal/vertical flips, rotations (\(\pm 15^\circ\)), affine and perspective transforms, color jitter, Gaussian blur, sharpness adjustment, and random erasing (p=0.5) with scales 2–20\% and aspect ratios 0.3–3.3.
\end{itemize}

\section{Methodology}

\subsection{Model Architecture}

Our approach employs a two-stage architecture combining segmentation-based preprocessing with classification. The core model utilizes EfficientNet-B3 as the backbone network, chosen for its optimal balance between accuracy and computational efficiency.

\textbf{Backbone Network:} EfficientNet-B3 pretrained on ImageNet provides robust feature extraction capabilities with approximately 12 million parameters. The architecture's compound scaling methodology ensures efficient use of model capacity across depth, width, and resolution dimensions.

\textbf{Classification Head:} The classifier consists of multiple fully connected layers with progressive dimensionality reduction:
\begin{itemize}
\item Adaptive global average pooling
\item Linear layer: 1536 → 1024 features
\item Batch normalization and ReLU activation
\item Dropout (p=0.5) for regularization
\item Linear layer: 1024 → 512 features
\item Final classification layer: 512 → 3 outputs
\end{itemize}

\textbf{Mask Integration:} A critical innovation in our approach is the integration of road segmentation masks during training. We employ a pretrained U-Net model with ResNet34 encoder to generate binary road masks, which focus the model's attention on relevant road areas and reduce background noise interference.

\subsection{Final Inference Design (Ensemble)}

The deployed pipeline in `inference\_pipeline/` uses a two-model ensemble with equal weights (0.5/0.5) and per-class thresholds (default 0.5):
\begin{itemize}
  \item \textbf{Model B} (best single model): EfficientNet-B3 backbone, \emph{no masks}, \emph{no CLAHE}, augmentation only.
  \item \textbf{Model H}: EfficientNet-B3 backbone with \emph{CLAHE} and \emph{partial mask} integration plus augmentation.
\end{itemize}

\begin{table}[h]
  \centering
  \begin{tabular}{lccc}
    \toprule
    \textbf{Variant} & \textbf{Masks} & \textbf{CLAHE} & \textbf{Macro F1} \\
    \midrule
    Model B & No & No & 0.806 \\
    Model H & Partial & Yes & 0.781 \\
    \bottomrule
  \end{tabular}
  \caption{Final ensemble components from `inference\_pipeline/config.yaml`.}
\end{table}

\subsection{Model B Architecture Diagram}

The final selected single model (Model B) uses EfficientNet-B3 features with a regularized MLP head (see `inference\_pipeline/src/model\_loader.py`).

\begin{figure}[htb]
  \centering
  \begin{tikzpicture}[
    node distance=5mm,
    box/.style={draw, rounded corners, align=center, minimum width=4.8cm, inner sep=3pt, font=\footnotesize},
    arr/.style={-Latex}
  ]
    \node[box] (in) {Input Image $256\times256\times3$};
    \node[box, below=of in] (backbone) {EfficientNet-B3 Backbone\\Features $\sim1536$};
    \node[box, below=of backbone] (fp) {Feature Processor\\AdaptiveAvgPool2d + Linear($1536\rightarrow512$)};
    \node[box, below=of fp] (cls) {Classifier Head\\Linear($512\rightarrow256\rightarrow128\rightarrow3$)};
    \node[box, below=of cls] (out) {Sigmoid $\rightarrow$ Multi-label Output};
    \draw[arr] (in) -- (backbone);
    \draw[arr] (backbone) -- (fp);
    \draw[arr] (fp) -- (cls);
    \draw[arr] (cls) -- (out);
  \end{tikzpicture}
  \caption{Model B architecture (no masks, no CLAHE).}
\end{figure}

\subsection{Training Configuration}

\textbf{Optimization Strategy:} We employ AdamW optimizer with a learning rate of 1e-3 and weight decay of 0.02. The learning rate schedule includes a warmup period covering 30\% of total training time, followed by cosine annealing for smooth convergence.

\textbf{Regularization Techniques:}
\begin{itemize}
\item Dropout with probability 0.5 in classification layers
\item Gradient clipping with maximum norm of 1.0
\item Early stopping with patience of 10 epochs
\item Mixed precision training (FP16) for memory efficiency
\end{itemize}

\textbf{Data Augmentation:} Our comprehensive augmentation pipeline includes:
\begin{itemize}
\item Geometric transformations: horizontal/vertical flips, rotation (±15°)
\item Color augmentations: brightness, contrast, saturation adjustments
\item Perspective transformations for viewpoint variation
\item Gaussian blur and sharpening for robustness
\item Random erasing (p=0.5) to prevent overfitting to specific regions
\end{itemize}

\subsection{Loss Function and Metrics}

Given the multi-label nature of our task, we employ binary cross-entropy loss for each classification head. Class weights are adjusted based on the inverse frequency of each class to handle imbalanced data distribution.

Evaluation metrics include accuracy, precision, recall, F1-score, and area under the ROC curve (AUC) for each classification task, providing comprehensive performance assessment across different aspects of model behavior.

\section{Results and Discussion}

\subsection{Overall Performance}

Our final model achieved exceptional performance across all classification tasks, with an overall accuracy of 88.99\%. The mask-enhanced approach provided a substantial 7.64\% improvement over baseline models without mask integration, demonstrating the effectiveness of focused feature learning on road regions.

\subsection{Task-Specific Results}

\textbf{Damage Detection:}
\begin{itemize}
\item Accuracy: 86.91\%
\item Precision: 83.99\%
\item Recall: 73.02\%
\item F1-Score: 78.12\%
\item ROC AUC: 0.923
\end{itemize}

The damage detection task showed strong precision but moderate recall, indicating conservative prediction behavior. The high ROC AUC of 0.923 demonstrates excellent discrimination ability between damaged and undamaged road sections.

\textbf{Occlusion Detection:}
\begin{itemize}
\item Accuracy: 94.11\%
\item Precision: 90.64\%
\item Recall: 77.43\%
\item F1-Score: 83.51\%
\item ROC AUC: 0.983
\end{itemize}

Occlusion detection achieved the most balanced performance across all metrics, with exceptional ROC AUC of 0.983. This task benefited significantly from the mask integration approach, as occlusions often involve background elements that masks help filter out.

\textbf{Crop Detection:}
\begin{itemize}
\item Accuracy: 98.07\%
\item Precision: 95.35\%
\item Recall: 55.41\%
\item F1-Score: 70.09\%
\item ROC AUC: 0.914
\end{itemize}

Crop detection showed the highest accuracy and precision but the lowest recall among all tasks. The high precision indicates that when the model predicts a cropped image, it is highly likely to be correct. However, the moderate recall suggests that some cropped images are missed, potentially due to the severe class imbalance (only 4.3\% positive examples).

\subsection{Training Evolution}

The training process demonstrated stable convergence with several key improvements implemented iteratively:

\begin{enumerate}
\item \textbf{Initial Model} (Accuracy: 81.35\%): Basic EfficientNet-B3 architecture with standard training configuration.
\item \textbf{Enhanced Model} (Accuracy: 89.16\%): Addition of batch normalization, increased dropout, and improved learning rate scheduling.
\item \textbf{Final Model} (Accuracy: 88.99\%): Integration of mask preprocessing and comprehensive augmentation pipeline.
\end{enumerate}

The slight decrease from enhanced to final model reflects improved generalization through more aggressive regularization and data augmentation.

\subsection{Ablation Studies}

Key findings from our experimental process:

\textbf{Architecture Comparison:} EfficientNet-B3 consistently outperformed ResNet50 by approximately 4.3\% in overall accuracy while using fewer parameters (12M vs 25M).

\textbf{Mask Integration Impact:} The road mask preprocessing provided the most significant single improvement (+7.64\% accuracy) by reducing false positives from background elements.

\textbf{Data Augmentation Effects:} Comprehensive augmentation strategies improved validation accuracy from 83.45\% (basic augmentations) to 89.16\% (full pipeline).

\subsection{Per-class Test Metrics}

\begin{table}[h]
  \centering
  \begin{tabular}{lccccc}
    \toprule
    \textbf{Class} & Acc. & Prec. & Rec. & F1 & ROC AUC \\
    \midrule
    Damage & 86.91 & 83.99 & 73.02 & 78.13 & 0.923 \\
    Occlusion & 94.11 & 90.64 & 77.43 & 83.51 & 0.983 \\
    Crop & 98.07 & 95.35 & 55.41 & 70.09 & 0.914 \\
    \bottomrule
  \end{tabular}
  \caption{Per-class test metrics (values from `data/visualization\_results/test\_metrics.json`). Percentages shown where applicable.}
\end{table}

\subsection{Key Figures}

\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.9\linewidth]{images/training_progress.png}
  \caption{Training/validation curves.}
\end{figure}

\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.9\linewidth]{images/roc_curves.png}
  \caption{ROC curves per class.}
\end{figure}

\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.9\linewidth]{images/precision_recall_curves.png}
  \caption{Precision-Recall curves per class.}
\end{figure}

\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.9\linewidth]{images/confusion_matrices.png}
  \caption{Confusion matrices for final model evaluation.}
\end{figure}

\section{Discussion}

\subsection{Strengths and Limitations}

Our approach demonstrates several key strengths: excellent performance in occlusion detection, high precision across all tasks, strong ROC AUC scores indicating good discrimination ability, and robust generalization to various image conditions. The mask-enhanced approach effectively addresses background noise interference and provides interpretable predictions by focusing on relevant road areas.

However, several limitations warrant attention. Damage detection recall (73.02\%) indicates that approximately 27\% of damaged road sections are missed by the model. This conservative behavior, while maintaining high precision, could impact the system's utility for comprehensive road monitoring. The severe class imbalance in crop detection (4.3\% positive examples) leads to suboptimal recall performance despite high precision.

\subsection{Computational Efficiency}

The final model achieves real-time performance with approximately 50ms inference time per image on an RTX 4070 Ti Super GPU. This efficiency makes the system suitable for practical deployment scenarios including mobile road monitoring applications and real-time traffic management systems.

\subsection{Practical Implications}

The system's high precision across all tasks makes it particularly suitable for automated road inspection systems where false alarms are costly. The strong performance in occlusion detection is especially valuable for autonomous vehicle applications where visibility assessment is critical for safe navigation.

\section{Conclusion and Future Work}

This project successfully developed a robust road distress classification system achieving 88.99\% overall accuracy through the integration of mask-enhanced feature learning with modern deep learning architectures. The system demonstrates strong performance across three critical road condition assessment tasks with particular excellence in occlusion detection.

Key technical contributions include the development of a mask-enhanced training approach that improved accuracy by 7.64\%, comprehensive data augmentation strategies optimized for road imagery, and creation of a complete inference pipeline suitable for real-world deployment. The system's real-time performance capability and high precision make it well-suited for practical road monitoring applications.

Future work should address the identified limitations through several avenues. Improving damage detection recall through class-specific augmentation strategies, collecting additional diverse damage examples, and implementing weighted loss functions could enhance sensitivity to damaged road conditions. Addressing the class imbalance in crop detection through advanced sampling techniques and threshold optimization would improve overall system robustness.

Architectural enhancements including attention mechanisms, transformer-based approaches, and ensemble methods offer potential for further performance improvements. Integration with temporal information from video sequences could provide additional context for more accurate distress classification. Finally, deployment optimization through model quantization and edge computing adaptations would enable broader practical adoption of the system.

The project provides a solid foundation for automated road infrastructure monitoring with clear pathways for continued improvement and real-world deployment optimization.

\clearpage

% Switch to one-column for References
\onecolumn
\bibliographystyle{plainnat}
\bibliography{references}

\twocolumn
\section{Appendix}

\subsection{Implementation Details}

The complete system was implemented in Python using PyTorch framework with the following key libraries:
\begin{itemize}
\item PyTorch and torchvision for deep learning implementation
\item segmentation-models-pytorch for U-Net architecture
\item OpenCV and PIL for image processing
\item scikit-learn for evaluation metrics
\item TensorBoard and Weights \& Biases for experiment tracking
\end{itemize}

\subsection{Hardware Configuration}

Training and evaluation were conducted on:
\begin{itemize}
\item GPU: NVIDIA RTX 4070 Ti Super (16GB VRAM)
\item CPU: Modern multi-core processor
\item RAM: 32GB system memory
\item Storage: High-speed SSD for data loading
\end{itemize}

\subsection{Hyperparameter Summary}

Final model hyperparameters:
\begin{itemize}
\item Batch size: 64
\item Learning rate: 1e-3 (initial)
\item Weight decay: 0.02
\item Dropout probability: 0.5
\item Image resolution: 256×256 pixels
\item Training epochs: 21 (early stopping)
\item Gradient clipping: 1.0
\end{itemize}

\end{document}
